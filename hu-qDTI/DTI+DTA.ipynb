{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from deepquantum import Circuit\n",
    "# from deepquantum.utils import dag,measure_state,ptrace,multi_kron,encoding,measure,expecval_ZI\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_num_drug = 64      #字典序\n",
    "embedding_num_target = 25    #字典序\n",
    "embedding_dim_drug = 16      #2^6\n",
    "embedding_dim_target = 16    #2^6\n",
    "hyber_para = 16              #2^4\n",
    "qubits_cirAorB = 4           #每一边的qubits数\n",
    "dim_embed = hyber_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Sequence\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import os\n",
    "\n",
    "from torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n",
    "VOCAB_PROTEIN = {\"A\": 1, \"C\": 2, \"B\": 3, \"E\": 4, \"D\": 5, \"G\": 6,\n",
    "                 \"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12,\n",
    "                 \"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18,\n",
    "                 \"U\": 19, \"T\": 20, \"W\": 21,\n",
    "                 \"V\": 22, \"Y\": 23, \"X\": 24,\n",
    "                 \"Z\": 25}\n",
    "VOCAB_LIGAND_ISO = {\"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "                    \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "                    \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "                    \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "                    \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "                    \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "                    \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "                    \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64}\n",
    "\n",
    "def str2int(ligand, protein, label):\n",
    "    ligand = [VOCAB_LIGAND_ISO[s] for s in ligand]\n",
    "    protein = [VOCAB_PROTEIN[s] for s in protein]\n",
    "        \n",
    "    if len(ligand) < 128:\n",
    "         ligand = np.pad(ligand, (0, 128 - len(ligand)))\n",
    "    else:\n",
    "        ligand = ligand[:128]\n",
    "    if len(protein) < 512:\n",
    "        protein = np.pad(protein, (0, 512 - len(protein)))\n",
    "    else:\n",
    "        protein = protein[:512]\n",
    "\n",
    "    return torch.tensor(ligand, dtype=torch.long), torch.tensor(protein, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "def Gram(data, hyber_para = 16):\n",
    "    data = data.T @ data\n",
    "    data = data.view(hyber_para, hyber_para)\n",
    "    return data\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_name, embed_prot=len(VOCAB_PROTEIN), embed_lig=len(VOCAB_LIGAND_ISO), is_Gram = False):\n",
    "        super(MyDataset).__init__()\n",
    "        self.embed_lig = nn.Embedding(embed_lig, 16, padding_idx=0)\n",
    "        self.embed_prot = nn.Embedding(embed_prot, 16, padding_idx=0)\n",
    "        self.is_Gram = is_Gram\n",
    "        self.path = os.path.join(root_dir, csv_name)\n",
    "        self.data = pd.read_csv(self.path)\n",
    "        self.embed_drug = nn.Embedding(embed_lig, embed_lig, padding_idx=0)\n",
    "        self.embed_target = nn.Embedding(embed_prot, embed_prot, padding_idx=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ligand , protein, label =  self.data.iloc[idx,]['smiles'], self.data.iloc[idx,]['sequence'], self.data.iloc[idx,]['label']\n",
    "        ligand, protein, label = str2int(ligand, protein, label)\n",
    "        if self.is_Gram:\n",
    "            ligand = Gram(self.embed_lig(ligand))\n",
    "            protein = Gram(self.embed_prot(protein))\n",
    "        return ligand, protein, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    root_dir = r'./data/'\n",
    "    train_name = 'training_dataset.csv'\n",
    "    train_dataset = MyDataset(root_dir, train_name)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "    print(len(VOCAB_LIGAND_ISO))\n",
    "    # for data in train_dataloader:\n",
    "    #     ligand, _, _ = data\n",
    "    #     print(ligand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.FC1 = nn.Linear(128,32)\n",
    "        self.FC2 = nn.Linear(32,1)\n",
    "    def forward(self,x):\n",
    "        out = F.leaky_relu(self.FC1(x))\n",
    "        out = F.leaky_relu(self.FC2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerBased(nn.Module):\n",
    "    def __init__(self, embedding_num_drug, embedding_num_target, embedding_dim_drug=dim_embed,\n",
    "                  embedding_dim_target=dim_embed, conv1_out_dim = qubits_cirAorB):\n",
    "        super().__init__()\n",
    "        # self.data_pre = ClassicalPre()\n",
    "        self.drugconv1d = nn.Conv1d(embedding_dim_drug, conv1_out_dim, kernel_size = 4, stride = 1, padding = 'same')\n",
    "        self.targetconv1d = nn.Conv1d(embedding_dim_target, conv1_out_dim, kernel_size = 4, stride = 1, padding = 'same')\n",
    "        self.linearlayer = Linear()\n",
    "    def forward(self,drug_input, target_input):   #x是一条记录\n",
    "        # drug_input, target_input = self.data_pre(x)\n",
    "        drug_output = self.drugconv1d(drug_input)   #1 4 16\n",
    "\n",
    "        target_output = self.targetconv1d(target_input)   #1 4 16\n",
    "\n",
    "        linear_input = torch.cat([drug_output, target_output],dim=1).view(drug_output.shape[0],-1)\n",
    "        # print('linear',linear_input.shape)\n",
    "        linear_output = self.linearlayer(linear_input)\n",
    "        affinity = linear_output\n",
    "        return affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Three 1d convolutional layer with relu activation stacked on top of each other\n",
    "    with a final global maxpooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, channel, kernel_size, stride=1, padding=0):\n",
    "        super(Conv1d, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=128)\n",
    "        self.conv1 = nn.Conv1d(128, channel, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv1d(channel, channel*2, kernel_size, stride, padding)\n",
    "        self.conv3 = nn.Conv1d(channel*2, channel*3, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.globalmaxpool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.globalmaxpool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class DeepDTA(nn.Module):\n",
    "    \"\"\"DeepDTA model architecture, Y-shaped net that does 1d convolution on \n",
    "    both the ligand and the protein representation and then concatenates the\n",
    "    result into a final predictor of binding affinity\"\"\"\n",
    "\n",
    "    def __init__(self, pro_vocab_size, lig_vocab_size, channel, protein_kernel_size, ligand_kernel_size):\n",
    "        super(DeepDTA, self).__init__()\n",
    "        self.ligand_conv = Conv1d(lig_vocab_size, channel, ligand_kernel_size)\n",
    "        self.protein_conv = Conv1d(pro_vocab_size, channel, protein_kernel_size)\n",
    "        self.fc1 = nn.Linear(channel*6, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, protein, ligand):\n",
    "        x1 = self.ligand_conv(ligand)\n",
    "        x2 = self.protein_conv(protein)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     root_dir = r'./data/'\n",
    "#     train_name = 'training_dataset.csv'\n",
    "#     train_dataset = MyDataset(root_dir, train_name)\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "#     model = DeepDTA(512,128,32,12,4)\n",
    "#     for data in train_dataloader:\n",
    "#         ligand, protein, _ = data\n",
    "#         pred = model(ligand, protein)\n",
    "#         print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_mse(records_real, records_predict):\n",
    "    \"\"\"\n",
    "    均方误差 估计值与真值 偏差\n",
    "    \"\"\"\n",
    "    if len(records_real) == len(records_predict):\n",
    "        return sum([(x - y) ** 2 for x, y in zip(records_real, records_predict)]) / len(records_real)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_rmse(records_real, records_predict):\n",
    "    \"\"\"\n",
    "    均方根误差：是均方误差的算术平方根\n",
    "    \"\"\"\n",
    "    mse = get_mse(records_real, records_predict)\n",
    "    if mse:\n",
    "        return math.sqrt(mse)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def cal_pccs(x, y, n):\n",
    "    \"\"\"\n",
    "    warning: data format must be narray\n",
    "    :param x: Variable 1\n",
    "    :param y: The variable 2\n",
    "    :param n: The number of elements in x\n",
    "    :return: pccs\n",
    "    \"\"\"\n",
    "    sum_xy = np.sum(np.sum(x*y))\n",
    "    sum_x = np.sum(np.sum(x))\n",
    "    sum_y = np.sum(np.sum(y))\n",
    "    sum_x2 = np.sum(np.sum(x*x))\n",
    "    sum_y2 = np.sum(np.sum(y*y))\n",
    "    pcc = (n*sum_xy-sum_x*sum_y)/np.sqrt((n*sum_x2-sum_x*sum_x)*(n*sum_y2-sum_y*sum_y))\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 6.743253702088819\n",
      "pccs 0.2817903143030842\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# pccs = pearsonr(x, y)\n",
    "\n",
    "def deepdta_val(model,dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    rmse_list = []\n",
    "    for data in dataloader:\n",
    "        ligand, protein, label =  data\n",
    "        exp = torch.tensor(label, dtype=torch.float).unsqueeze(-1).detach().numpy()\n",
    "        pred = model(ligand, protein).detach().numpy()\n",
    "        rmse_list.append(get_rmse(exp, pred))\n",
    "        pred_list.append(pred)\n",
    "        label_list.append(exp)\n",
    "    rmse = sum(rmse_list) / len(rmse_list)\n",
    "    # rmse = get_rmse(label_list, pred_list)\n",
    "    \n",
    "    # print('pred_list:', pred_list[0].shape)\n",
    "    pred = np.concatenate(pred_list).reshape(-1)\n",
    "    label = np.concatenate(label_list).reshape(-1)\n",
    "    # print('pred:',pred.shape)\n",
    "    # print('label', label.shape)\n",
    "    # print(0 in pred)\n",
    "    # print(0 in label)\n",
    "    pccs = np.corrcoef(pred, label)[0, 1]\n",
    "    # pccs = cal_pccs(label_list, pred_list, len(label_list))\n",
    "    # pccs = np.column_stack(label_list, pred_list)\n",
    "    # pccs = sum(pccs_list) / len(pccs_list)\n",
    "    \n",
    "    return rmse, pccs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_dir = r'./data/'\n",
    "    train_name = 'test_dataset.csv'\n",
    "    train_dataset = MyDataset(root_dir, train_name)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "    model = DeepDTA(512,128,32,12,4)\n",
    "    rmse, pccs = deepdta_val(model=model, dataloader=train_dataloader)\n",
    "    print('rmse', rmse)\n",
    "    print('pccs', pccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepDTA + MyDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 处理np计算过程中遇到0除以0的情况\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "\n",
    "root_dir = r'./data/'\n",
    "train_dataset_name = 'training_dataset.csv'\n",
    "test_dataset_name = 'test_dataset.csv'\n",
    "val_dataset_name = 'validation_dataset.csv'\n",
    "TRAIN_PATH = './data/DTA_result/train/6-14/epoch{} train_loss_min_{}_dict_dta.pth'\n",
    "TEST_PATH = './data/DTA_result/test/6-14/epoch{}_dict_dta.pth'\n",
    "BEST_RESULT_PATH = './data/DTA_result/best_result_dict_dta.pth'\n",
    "\n",
    "train_dataset = MyDataset(root_dir=root_dir, csv_name=train_dataset_name)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "test_dataset = MyDataset(root_dir=root_dir, csv_name=test_dataset_name)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "val_dataset = MyDataset(root_dir=root_dir, csv_name=val_dataset_name)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "model = DeepDTA(512,128,32,12,4)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "epochs = 600\n",
    "\n",
    "loss_list = []\n",
    "train_loss_list = []\n",
    "test_rmse_list = []\n",
    "test_pr_list = []\n",
    "\n",
    "train_min_loss = 1000\n",
    "test_min_rmse = 1000\n",
    "test_max_pr = -1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in train_dataloader:\n",
    "        ligand, protein, label = data\n",
    "        pred = model(ligand, protein)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, label)\n",
    "        loss_list.append(loss.detach().numpy().tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = sum(loss_list) / len(loss_list)\n",
    "    if train_loss < train_min_loss:\n",
    "        train_min_loss = train_loss\n",
    "        torch.save(model.state_dict(), TRAIN_PATH.format(str(epoch),str(train_loss)[7:13]))\n",
    "\n",
    "    # start validatin\n",
    "    test_rmse, test_pr = deepdta_val(model, test_dataloader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_rmse_list.append(test_rmse)\n",
    "    test_pr_list.append(test_pr)\n",
    "    \n",
    "    if test_rmse < test_min_rmse or test_pr > test_max_pr:\n",
    "        if test_rmse < test_min_rmse and test_pr > test_max_pr:\n",
    "            test_min_rmse = test_rmse\n",
    "            test_max_pr = test_pr\n",
    "            torch.save(model.state_dict(), BEST_RESULT_PATH)\n",
    "            print('best result-epochs:', epoch + 1, 'train-loss:', '%.4f' % train_loss)\n",
    "            print('valid-rmse:', '% .4f'%test_rmse, 'valid-pr:','% .4f'%test_pr)\n",
    "            continue\n",
    "        elif test_rmse < test_min_rmse :\n",
    "            test_min_rmse = test_rmse\n",
    "        else:\n",
    "            test_max_pr = test_pr\n",
    "        torch.save(model.state_dict(), TEST_PATH.format(str(epoch)))\n",
    "    \n",
    "\n",
    "    print('epochs:', epoch + 1, 'train-loss:', '%.4f' % train_loss)\n",
    "    print('valid-rmse:', '% .4f'%test_rmse, 'valid-pr:','% .4f'%test_pr)\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_RESULT_PATH)) \n",
    "val_rmse, val_pr = deepdta_val(model, val_dataloader)\n",
    "print('valid-rmse:', '% .4f'%val_rmse, 'valid-pr:','% .4f'%val_pr)\n",
    "\n",
    "save_path = \"./data/DTA_result/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "dict = {\"train_loss\": train_loss_list,\"rmse\":test_rmse_list, \"pr\":test_pr_list}\n",
    "with open(save_path + \"DTA_result.json\", \"w\") as f:\n",
    "    json.dump(dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + MyDataset\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "\n",
    "root_dir = r'./data/'\n",
    "train_dataset_name = 'training_dataset.csv'\n",
    "test_dataset_name = 'test_dataset.csv'\n",
    "val_dataset_name = 'validation_dataset.csv'\n",
    "TRAIN_PATH = './data/DTI_result/6-16/train/epoch{} train_loss_min_{}_dict_cnn.pth'\n",
    "TEST_PATH = './data/DTI_result/6-16/test/epoch{}_dict_cnn.pth'\n",
    "BEST_RESULT_PATH = './data/DTI_result/best_result_dict_cnn.pth'\n",
    "\n",
    "train_dataset = MyDataset(root_dir=root_dir, csv_name=train_dataset_name, is_Gram=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "test_dataset = MyDataset(root_dir=root_dir, csv_name=test_dataset_name, is_Gram=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "val_dataset = MyDataset(root_dir=root_dir, csv_name=val_dataset_name, is_Gram=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32,shuffle=True,drop_last=True)\n",
    "\n",
    "model = CNNLayerBased(64,25)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 500\n",
    "\n",
    "loss_list = []\n",
    "train_loss_list = []\n",
    "test_rmse_list = []\n",
    "test_pr_list = []\n",
    "\n",
    "train_min_loss = 1000\n",
    "test_min_rmse = 1000\n",
    "test_max_pr = -1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in train_dataloader:\n",
    "        ligand, protein, label = data\n",
    "        # print(ligand.shape)\n",
    "        # ligand = np.squeeze(ligand)\n",
    "        # protein = np.squeeze(protein)\n",
    "        # print(ligand.shape)\n",
    "        pred = model(ligand, protein)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, label)\n",
    "        loss_list.append(loss.detach().numpy().tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = sum(loss_list) / len(loss_list)\n",
    "    if train_loss < train_min_loss:\n",
    "        train_min_loss = train_loss\n",
    "        torch.save(model.state_dict(), TRAIN_PATH.format(str(epoch),str(train_loss)[7:13]))\n",
    "\n",
    "    # start validatin\n",
    "    test_rmse, test_pr = deepdta_val(model, test_dataloader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_rmse_list.append(test_rmse)\n",
    "    test_pr_list.append(test_pr)\n",
    "    \n",
    "    if test_rmse < test_min_rmse or test_pr > test_max_pr:\n",
    "        if test_rmse < test_min_rmse and test_pr > test_max_pr:\n",
    "            test_min_rmse = test_rmse\n",
    "            test_max_pr = test_pr\n",
    "            torch.save(model.state_dict(), BEST_RESULT_PATH)\n",
    "            print('best result-epochs:', epoch + 1, 'train-loss:', '%.4f' % train_loss)\n",
    "            print('valid-rmse:', '% .4f'%test_rmse, 'valid-pr:','% .4f'%test_pr)\n",
    "            continue\n",
    "        elif test_rmse < test_min_rmse :\n",
    "            test_min_rmse = test_rmse\n",
    "        else:\n",
    "            test_max_pr = test_pr\n",
    "        torch.save(model.state_dict(), TEST_PATH.format(str(epoch)))\n",
    "    \n",
    "\n",
    "    print('epochs:', epoch + 1, 'train-loss:', '%.4f' % train_loss)\n",
    "    print('valid-rmse:', '% .4f'%test_rmse, 'valid-pr:','% .4f'%test_pr)\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_RESULT_PATH)) \n",
    "val_rmse, val_pr = deepdta_val(model, val_dataloader)\n",
    "print('valid-rmse:', '% .4f'%val_rmse, 'valid-pr:','% .4f'%val_pr)\n",
    "\n",
    "save_path = \"./data/DTI_result/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "dict = {\"train_loss\": train_loss_list,\"rmse\":test_rmse_list, \"pr\":test_pr_list}\n",
    "with open(save_path + \"DTI_result_epoch500.json\", \"w\") as f:\n",
    "    json.dump(dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiAAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
